{"cells":[{"cell_type":"markdown","metadata":{"id":"3gT05gnqh-1C"},"source":["# Training\n","\n","In this chapter we will use all the stuff we learned to finally train a neural network ðŸ¤– using neuroimaging data ðŸ§ \n","\n","So, get ready for your new favorite hobby ðŸ¤“\n","\n","<p>\n","<img src=\"https://programmerhumor.io/wp-content/uploads/2023/05/programmerhumor-io-python-memes-programming-memes-d917ae7c7cb4095-758x495.jpg\" width=500/>\n","<figcaption>Taken from <a href=\"https://programmerhumor.io/python-memes/its-been-a-decade/\">https://programmerhumor.io/python-memes/its-been-a-decade/</a></figcaption>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"IIsX7HnjKXrc"},"source":["## 1. What is training?\n","\n","During model training (the sentence continues here ðŸ‘‡)\n","\n","```python\n","...\n","neural_net = NeuralNet()          # the neural net              \n","for nifti, age in dl:             # is provided with training input\n","  nifti_age = neural_net(nifti)   # and the neural nets output\n","  error = (age - nifti_age) ** 2  # is compared to the desired output\n","  error.backward()                # and the net is adjusted (using Autograd ðŸ¦®)\n","```\n","Wow, this code is already pretty close to our goal (3 more lines to make it work). We are only missing one final ingredient: The optimizer!\n","\n","## 2. `torch.nn.optim`\n","\n","The module `torch.nn.optim` provides a collection of optimizers which handle the adjustment of (neural net-) parameters.\n","\n","First the optimizer is given the parameters it should update\n","\n","```python\n","...   \n","neural_net = NeuralNet()\n","optimizer = torch.optim.Adam(neural_net.parameters())  # creating the optimizer\n","```\n","then it can be applied in the above code by adding one line at the beginning and one at the end of the training loop\n","\n","```python          \n","for nifti, age in dl:\n","  optimizer.zero_grad()           # resetting the parameters gradients to zero\n","  nifti_age = neural_net(nifti)\n","  error = (age - nifti_age) ** 2\n","  error.backward()\n","  optimizer.step()                # adjusting the parameters (using Autograd ðŸ¦®)\n","```\n","Those are the three missing lines which make the above code actually work.\n","\n","The two most common ways to tinker with the optimizer are by\n","1. changing the learning rate to e.g. `1e-2` via `torch.optim.Adam(neural_net.parameters(), lr=1e-2)`\n","2. changing the optimizer e.g. using `torch.optim.SGD(...)` instead of `torch.optim.Adam(...)`\n","\n","`torch.optim.SGD(...)` - Stochastic Gradient Descent - is simply changing the parameters by $lr * -\\nabla$ (re-read 5_PyTorch, 2. Autograd ðŸ¦®)\n","\n","`torch.optim.Adam(...)` is typically **your optimizer of choice** (doing [a bit more](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html))  working good with `lr` between `1e-5` and `1e-1`\n"]},{"cell_type":"markdown","metadata":{"id":"4FIbV6KpZA78"},"source":["# Exercise\n","\n","## ðŸš¨ Warning ðŸš¨\n","\n","This Notebook builds on 1_Introduction and the exercise of 2_Data_Exploration and 4_Preprocessing.\n","\n","You have to run these Notebooks (if you didn't already) and mount your Google Drive to this Notebook via\n","```python\n","from google.colab import drive\n","drive.mount('/content/drive')\n","```\n","then you are ready to go!"]},{"cell_type":"markdown","metadata":{"id":"k-kriXcW9AzT"},"source":["1. Use the exercise solution of 6_Convolutional_Neural_Net and add the training loop shown in chapter 2 `torch.nn.optim`!"]},{"cell_type":"markdown","metadata":{"id":"_7hv8ckzCwph"},"source":["2. Print `error`, `nifti_age` and `age` inside the training loop!"]},{"cell_type":"markdown","metadata":{"id":"9a8YpkkXD7Gn"},"source":["3. Use the exercise solution of 5_PyTorch to **create separate training and validation DataLoaders**."]},{"cell_type":"markdown","metadata":{"id":"kKIGF4iJEoQt"},"source":["4. Run the training loop (with print statements) **5 times** - using an additional `for` loop - with the **training DataLoader**!"]},{"cell_type":"markdown","metadata":{"id":"-K5CL3q1Hi5Y"},"source":["5. Use the **trained `neural_net`** to run the loop (with print statements) **one time** with the **validation DataLoader**. **Delete/Uncomment all lines which could adjust the neural nets parameters** as we do not want to train during validation."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}