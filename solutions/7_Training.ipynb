{"cells":[{"cell_type":"markdown","metadata":{"id":"3gT05gnqh-1C"},"source":["# Training\n","\n","In this chapter we will use all the stuff we learned to finally train a neural network ðŸ¤– using neuroimaging data ðŸ§ \n","\n","So, get ready for your new favorite hobby ðŸ¤“\n","\n","<p>\n","<img src=\"https://programmerhumor.io/wp-content/uploads/2023/05/programmerhumor-io-python-memes-programming-memes-d917ae7c7cb4095-758x495.jpg\" width=500/>\n","<figcaption>Taken from <a href=\"https://programmerhumor.io/python-memes/its-been-a-decade/\">https://programmerhumor.io/python-memes/its-been-a-decade/</a></figcaption>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"IIsX7HnjKXrc"},"source":["## 1. What is training?\n","\n","During model training (the sentence continues here ðŸ‘‡)\n","\n","```python\n","...\n","neural_net = NeuralNet()          # the neural net              \n","for nifti, age in dl:             # is provided with training input\n","  nifti_age = neural_net(nifti)   # and the neural nets output\n","  error = (age - nifti_age) ** 2  # is compared to the desired output\n","  error.backward()                # and the net is adjusted (using Autograd ðŸ¦®)\n","```\n","Wow, this code is already pretty close to our goal (3 more lines to make it work). We are only missing one final ingredient: The optimizer!\n","\n","## 2. `torch.nn.optim`\n","\n","The module `torch.nn.optim` provides a collection of optimizers which handle the adjustment of (neural net-) parameters.\n","\n","First the optimizer is given the parameters it should update\n","\n","```python\n","...   \n","neural_net = NeuralNet()\n","optimizer = torch.optim.Adam(neural_net.parameters())  # creating the optimizer\n","```\n","then it can be applied in the above code by adding one line at the beginning and one at the end of the training loop\n","\n","```python          \n","for nifti, age in dl:\n","  optimizer.zero_grad()           # resetting the parameters gradients to zero\n","  nifti_age = neural_net(nifti)\n","  error = (age - nifti_age) ** 2\n","  error.backward()\n","  optimizer.step()                # adjusting the parameters (using Autograd ðŸ¦®)\n","```\n","Those are the three missing lines which make the above code actually work.\n","\n","The two most common ways to tinker with the optimizer are by\n","1. changing the learning rate to e.g. `1e-2` via `torch.optim.Adam(neural_net.parameters(), lr=1e-2)`\n","2. changing the optimizer e.g. using `torch.optim.SGD(...)` instead of `torch.optim.Adam(...)`\n","\n","`torch.optim.SGD(...)` - Stochastic Gradient Descent - is simply changing the parameters by $lr * -\\nabla$ (re-read 5_PyTorch, 2. Autograd ðŸ¦®)\n","\n","`torch.optim.Adam(...)` is typically **your optimizer of choice** (doing [a bit more](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html))  working good with `lr` between `1e-5` and `1e-1`\n"]},{"cell_type":"markdown","metadata":{"id":"4FIbV6KpZA78"},"source":["# Exercise\n","\n","## ðŸš¨ Warning ðŸš¨\n","\n","This Notebook builds on 1_Introduction and the exercise of 2_Data_Exploration and 4_Preprocessing.\n","\n","You have to run these Notebooks (if you didn't already) and mount your Google Drive to this Notebook via\n","```python\n","from google.colab import drive\n","drive.mount('/content/drive')\n","```\n","then you are ready to go!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16113,"status":"ok","timestamp":1707319702021,"user":{"displayName":"Lukas Fisch","userId":"00299531097189765345"},"user_tz":-60},"id":"c4kRGzGg9zZq","outputId":"cb5c66b0-b161-4ae2-e1bd-5425829acb05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"k-kriXcW9AzT"},"source":["1. Use the exercise solution of 6_Convolutional_Neural_Net and add the training loop shown in chapter 2 `torch.nn.optim`!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTy7jTkv9xiU"},"outputs":[],"source":["import torch\n","import numpy as np\n","import nibabel as nib\n","import torch.nn.functional as F\n","\n","class NiftiDataset(torch.utils.data.Dataset):\n","    def __init__(self, filepaths, age_array, size=(128, 128, 128)):\n","        self.niftis = [self.load_nifti(fpath, size) for fpath in filepaths]\n","        self.ages = torch.from_numpy(age_array)\n","\n","    def __len__(self):\n","        return len(self.niftis)\n","\n","    def __getitem__(self, idx):\n","        return self.niftis[idx], self.ages[idx].float()\n","\n","    @staticmethod\n","    def load_nifti(filepath, size):\n","      img = nib.load(filepath)\n","      img = nib.as_closest_canonical(img)\n","      x = img.get_fdata(dtype=np.float32)\n","      x = torch.from_numpy(x)\n","      x = x[None, None]  # added 2 dimensions using [None, None] to make it 5D\n","      x = F.interpolate(x, size=size)  # resized it to (1, 1, 128, 128, 128)\n","      return x[0]  # Removed one dimension using [0] because DataLoader will add one later"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYpTH_c58X7O"},"outputs":[],"source":["import torch.nn as nn\n","\n","class NeuralNet(nn.Module):\n","  def __init__(self, kernel_size=3):\n","    super().__init__()\n","    self.conv1 = nn.Conv3d(1, 4, kernel_size=kernel_size)\n","    self.pool1 = nn.MaxPool3d(kernel_size=kernel_size)\n","    self.conv2 = nn.Conv3d(4, 16, kernel_size=kernel_size)\n","    self.pool2 = nn.MaxPool3d(kernel_size=kernel_size)\n","    self.conv3 = nn.Conv3d(16, 32, kernel_size=kernel_size)\n","    self.pool3 = nn.MaxPool3d(kernel_size=kernel_size)\n","    self.conv4 = nn.Conv3d(32, 1, kernel_size=kernel_size)\n","\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = self.pool1(x)\n","    x = self.conv2(x)\n","    x = self.pool2(x)\n","    x = self.conv3(x)\n","    x = self.pool3(x)\n","    x = self.conv4(x)\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXg5VXhg9xbT"},"outputs":[],"source":["import pandas as pd\n","PATH = '/content/drive/MyDrive/openneuro'\n","\n","df = pd.read_csv(f'{PATH}/dataframe_after_preprocessing.csv', index_col=0)\n","ds = NiftiDataset(df.zscore_filepath, age_array=df.age.values)\n","dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"_7hv8ckzCwph"},"source":["2. Print `error`, `nifti_age` and `age` inside the training loop!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6324,"status":"ok","timestamp":1707319741572,"user":{"displayName":"Lukas Fisch","userId":"00299531097189765345"},"user_tz":-60},"id":"c7uDv95W9xQR","outputId":"05e08272-cb51-4e41-9817-a958b2d2b23d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[[679.8942]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[-0.0748]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[456.3232]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[0.6383]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[383.3004]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[1.4219]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[563.4132]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[2.2637]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[229.5711]]]]], grad_fn=<PowBackward0>) tensor([19.]) tensor([[[[[3.8484]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[208.2583]]]]], grad_fn=<PowBackward0>) tensor([20.]) tensor([[[[[5.5688]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[258.8898]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[7.9099]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[200.8020]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[9.8295]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[227.9518]]]]], grad_fn=<PowBackward0>) tensor([30.]) tensor([[[[[14.9019]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[7.1048]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[19.3345]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[17.9766]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[28.2399]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[53.9742]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[33.3467]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[281.0528]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[37.7646]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[22.9438]]]]], grad_fn=<PowBackward0>) tensor([27.]) tensor([[[[[31.7900]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[17.5531]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[28.1896]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[6.5094]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[23.5513]]]]], grad_fn=<ConvolutionBackward0>)\n"]}],"source":["neural_net = NeuralNet(kernel_size=3)\n","optimizer = torch.optim.Adam(neural_net.parameters(), lr=1e-3)\n","\n","for nifti, age in dl:\n","  optimizer.zero_grad()\n","  nifti_age = neural_net(nifti)\n","  error = (age - nifti_age) ** 2\n","  print(error, age, nifti_age)\n","  error.backward()\n","  optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"9a8YpkkXD7Gn"},"source":["3. Use the exercise solution of 5_PyTorch to **create separate training and validation DataLoaders**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eVel3n9Esf_"},"outputs":[],"source":["valid_pids = [f'sub-01', 'sub-02', 'sub-03']\n","\n","train_df = df[~df.index.isin(valid_pids)]\n","valid_df = df[df.index.isin(valid_pids)]\n","\n","train_ds = NiftiDataset(train_df.t1w_filepath, age_array=train_df.age.values)\n","valid_ds = NiftiDataset(valid_df.t1w_filepath, age_array=valid_df.age.values)\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True)\n","valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=1, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"kKIGF4iJEoQt"},"source":["4. Run the training loop (with print statements) **5 times** - using an additional `for` loop - with the **training DataLoader**!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20188,"status":"ok","timestamp":1707319763328,"user":{"displayName":"Lukas Fisch","userId":"00299531097189765345"},"user_tz":-60},"id":"ZOTiei76Es-g","outputId":"2bf8a28d-09f7-440d-84f4-9c7627a4a890"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[[1016.8278]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[55.8877]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[12580.0459]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[-90.1608]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[1967.8070]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[-18.3600]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[28.9679]]]]], grad_fn=<PowBackward0>) tensor([20.]) tensor([[[[[25.3822]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4496.9248]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[88.0591]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4871.1641]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[91.7937]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.4453]]]]], grad_fn=<PowBackward0>) tensor([30.]) tensor([[[[[29.3327]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[366.9161]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[1.8449]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[636.1960]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[-1.2229]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[1276.3409]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[-11.7259]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[1002.7175]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[-10.6657]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[1211.8422]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[-8.8115]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[240.1368]]]]], grad_fn=<PowBackward0>) tensor([19.]) tensor([[[[[3.5037]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[67.6045]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[32.2222]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[398.9554]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[41.9739]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[17.4821]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[16.8188]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[95.8506]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[30.7903]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[358.9663]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[44.9464]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[297.5033]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[41.2483]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[352.5239]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[44.7756]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[38.2598]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[17.8145]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[78.6507]]]]], grad_fn=<PowBackward0>) tensor([19.]) tensor([[[[[27.8685]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[142.8508]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[32.9520]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[204.1491]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[7.7119]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[103.6300]]]]], grad_fn=<PowBackward0>) tensor([30.]) tensor([[[[[19.8201]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[6.4984]]]]], grad_fn=<PowBackward0>) tensor([20.]) tensor([[[[[17.4508]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[15.0489]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[24.8793]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[131.6505]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[9.5261]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[28.9347]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[20.6209]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[147.8949]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[11.8388]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4.0088]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[23.9978]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[7.9106]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[21.1874]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[98.9962]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[31.9497]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.0509]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[20.7744]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[5.9559]]]]], grad_fn=<PowBackward0>) tensor([19.]) tensor([[[[[21.4405]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[307.7991]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[41.5442]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.6927]]]]], grad_fn=<PowBackward0>) tensor([20.]) tensor([[[[[20.8323]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[64.1839]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[13.9885]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4.8759]]]]], grad_fn=<PowBackward0>) tensor([30.]) tensor([[[[[27.7918]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.6087]]]]], grad_fn=<PowBackward0>) tensor([19.]) tensor([[[[[18.2198]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[3.8203]]]]], grad_fn=<PowBackward0>) tensor([30.]) tensor([[[[[28.0454]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[23.3458]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[26.8317]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4.6289]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[23.8485]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[2.7830]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[19.3318]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[6.6922]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[21.4131]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[43.5790]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[30.6014]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[19.9679]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[21.5315]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4.9517]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[23.2252]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[21.6265]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[19.3496]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[8.3915]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[18.1032]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.1528]]]]], grad_fn=<PowBackward0>) tensor([20.]) tensor([[[[[19.6091]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[5.0781]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[19.7465]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[9.8615]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[24.1403]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[1.0517]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[19.9745]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.0429]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[24.2070]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[6.4904]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[24.5476]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[6.4673]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[26.5431]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.0088]]]]], grad_fn=<PowBackward0>) tensor([19.]) tensor([[[[[18.9060]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.4069]]]]], grad_fn=<PowBackward0>) tensor([20.]) tensor([[[[[20.6379]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[6.3614]]]]], grad_fn=<PowBackward0>) tensor([21.]) tensor([[[[[23.5222]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[1.9231]]]]], grad_fn=<PowBackward0>) tensor([30.]) tensor([[[[[31.3867]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[1.1219]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[24.9408]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[0.3864]]]]], grad_fn=<PowBackward0>) tensor([22.]) tensor([[[[[21.3784]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4.7548]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[23.8195]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[3.8595]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[22.0354]]]]], grad_fn=<ConvolutionBackward0>)\n"]}],"source":["neural_net = NeuralNet(kernel_size=3)\n","optimizer = torch.optim.Adam(neural_net.parameters(), lr=1e-3)\n","\n","\n","for epoch in range(5):\n","  for nifti, age in train_dl:\n","    optimizer.zero_grad()\n","    nifti_age = neural_net(nifti)\n","    error = (age - nifti_age) ** 2\n","    print(error, age, nifti_age)\n","    error.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"-K5CL3q1Hi5Y"},"source":["5. Use the **trained `neural_net`** to run the loop (with print statements) **one time** with the **validation DataLoader**. **Delete/Uncomment all lines which could adjust the neural nets parameters** as we do not want to train during validation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":607,"status":"ok","timestamp":1707319763909,"user":{"displayName":"Lukas Fisch","userId":"00299531097189765345"},"user_tz":-60},"id":"iWOu-_euGPtU","outputId":"421da796-135b-4a22-e3a4-4982fdcd47d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[[52.1319]]]]], grad_fn=<PowBackward0>) tensor([26.]) tensor([[[[[18.7798]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[4.8318]]]]], grad_fn=<PowBackward0>) tensor([24.]) tensor([[[[[26.1981]]]]], grad_fn=<ConvolutionBackward0>)\n","tensor([[[[[25.4240]]]]], grad_fn=<PowBackward0>) tensor([27.]) tensor([[[[[21.9578]]]]], grad_fn=<ConvolutionBackward0>)\n"]}],"source":["  for nifti, age in valid_dl:\n","    # optimizer.zero_grad()\n","    nifti_age = neural_net(nifti)\n","    error = (age - nifti_age) ** 2\n","    print(error, age, nifti_age)\n","    # error.backward()\n","    # optimizer.step()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}